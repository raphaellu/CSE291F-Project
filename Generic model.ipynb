{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#import igraph\n",
    "import csv\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "#with open(\"testing_set.txt\", \"r\") as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    testing_set  = list(reader)\n",
    "\n",
    "#testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calcualte inner product\n",
    "def inner(X,Y):\n",
    "    s = 0\n",
    "    for i in range(0,len(X)):\n",
    "        s = s + X[i]*Y[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amazon-meta_item_item_0.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"amazon-meta_item_info.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f,delimiter='^')\n",
    "    node_info  = list(reader)\n",
    "\n",
    "node_info = node_info[1:]\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "title = [element[1] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"amazon-meta_filtered_ground_truth.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \")[0] for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_list = []\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i]\n",
    "    for j in xrange(len(testing_set)):\n",
    "        target = testing_set[j]\n",
    "        testing_list.append([source,target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107416\n",
      "107416\n"
     ]
    }
   ],
   "source": [
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set))))\n",
    "training_set_reduced_total = [training_set[i] for i in to_keep]\n",
    "#create a local test set\n",
    "training_set_reduced = training_set_reduced_total[:len(training_set_reduced_total)/2]\n",
    "testing_set = training_set_reduced_total[len(training_set_reduced_total)/2:]\n",
    "print(len(training_set_reduced))\n",
    "print(len(testing_set))\n",
    "#training_set_reduced = training_set_reduced_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in graph, each node represents a paper\n",
    "training_set_reduced_total = [training_set[i] for i in to_keep]\n",
    "G = nx.Graph()\n",
    "for t in training_set_reduced_total:\n",
    "    G.add_node(t[0])\n",
    "    G.add_node(t[1])\n",
    "    if t[2] == '1':\n",
    "        G.add_edge(t[0],t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# title TFIDF cos similarity\n",
    "title_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# calculate title TFIDF cos similarity\n",
    "    source_title_tfidf = features_TFIDF[index_source].toarray()[0]\n",
    "    target_title_tfidf = features_TFIDF[index_target].toarray()[0]\n",
    "    \n",
    "    cos = np.dot(source_title_tfidf,target_title_tfidf) / (np.linalg.norm(np.array(source_title_tfidf))\\\n",
    "       *np.linalg.norm(np.array(target_title_tfidf)))\n",
    "    title_similarity.append(cos)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# group is the same or not\n",
    "group_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    #group similarity true or false\n",
    "    group_source = source_info[2]\n",
    "    group_target = target_info[2]\n",
    "    \n",
    "    if group_source == group_target:\n",
    "        group_similarity.append(1)\n",
    "    else:\n",
    "        group_similarity.append(0)\n",
    "        \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of reviews\n",
    "reviews_num_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # number of reviews similarity\n",
    "    review_source = source_info[4]\n",
    "    review_target = target_info[4]\n",
    "    \n",
    "    reviews_num_similarity.append(np.absolute(int(review_source)-int(review_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# same number of words in detailed category\n",
    "category_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # same number of words in detailed category\n",
    "    category_source = source_info[6+int(review_source)*4:]\n",
    "    category_target = target_info[6+int(review_target)*4:]\n",
    "    \n",
    "    category_similarity.append(len(set(category_source).intersection(set(category_target))))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# rating \n",
    "rating_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # rating similarity\n",
    "    rating_source = source_info[5]\n",
    "    rating_target = target_info[5]\n",
    "    \n",
    "    rating_similarity.append(np.absolute(float(rating_source)-float(rating_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum of nodes' degrees \n",
    "node_degree = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    #add node degree\n",
    "    source_degree = G.degree(source)\n",
    "    target_degree = G.degree(target)\n",
    "    if isinstance(source_degree,dict):\n",
    "        source_degree = 0\n",
    "    if isinstance(target_degree,dict):\n",
    "        target_degree = 0\n",
    "    node_degree.append(source_degree+target_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distance between two nodes in graph\n",
    "dist_node = []\n",
    "\n",
    "counter = 0\n",
    "node_set = G.nodes()\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    if (source in node_set) and (target in node_set) and nx.has_path(G,source,target):\n",
    "        dist_node.append(-nx.shortest_path_length(G,source,target))\n",
    "    else:\n",
    "        dist_node.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of common neighbors\n",
    "common_neighbors = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    #add node degree\n",
    "    common_neighbors.append(len(list(nx.common_neighbors(G,source,target))))\n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute betweenness centralities for each node\n",
    "# since this is a very slow computation, for each node i , we pick 1600 (5% of len(training_set_reduced) \n",
    "# other nodes to compute the betweenness centrality of i.\n",
    "all_btw_centralities = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference between betweenness-centralities of target and source\n",
    "# idea comes from https://www.researchgate.net/publication/220432611_Link_Prediction_in_Citation_Networks, page 80-81\n",
    "btw_centralities = []\n",
    "\n",
    "counter = 0\n",
    "node_set = G.nodes()\n",
    "\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    if (source in all_btw_centralities) and (target in all_btw_centralities):\n",
    "        btw_centralities.append(all_btw_centralities[target] - all_btw_centralities[source])\n",
    "    else:\n",
    "        btw_centralities.append(0.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array([title_similarity, group_similarity, category_similarity,reviews_num_similarity,rating_similarity,node_degree,dist_node,common_neighbors]).T\n",
    "#training_features = np.array([overlap_title_tm, temp_diff_tm, comm_auth_tm, text_mining]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random prediction\n",
    "predictions_random = np.random.choice([0, 1], size=len(tmp_test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC(class_weight='balanced')\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-77cd3ce6cbfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.SVC()\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree regressor\n",
    "classifier = DecisionTreeRegressor(max_depth=2)\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02869666  0.          0.02202334  0.00493384  0.00290092  0.01875185\n",
      " -0.39529756  0.14019384]\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "theta = np.linalg.lstsq(training_features, labels_array)[0]\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "40001 testing examples processsed\n",
      "50001 testing examples processsed\n",
      "60001 testing examples processsed\n",
      "70001 testing examples processsed\n",
      "80001 testing examples processsed\n",
      "90001 testing examples processsed\n",
      "100001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "title_similarity_test = []\n",
    "group_similarity_test = []\n",
    "reviews_num_similarity_test = []\n",
    "category_similarity_test = []\n",
    "rating_similarity_test = []\n",
    "   \n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "#for i in range(50):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# calculate title TFIDF cos similarity\n",
    "    source_title_tfidf = features_TFIDF[index_source].toarray()[0]\n",
    "    target_title_tfidf = features_TFIDF[index_target].toarray()[0]\n",
    "    \n",
    "    cos = np.dot(source_title_tfidf,target_title_tfidf) / (np.linalg.norm(np.array(source_title_tfidf))\\\n",
    "       *np.linalg.norm(np.array(target_title_tfidf)))\n",
    "    title_similarity_test.append(cos)\n",
    "    \n",
    "    #group similarity true or false\n",
    "    group_source = source_info[2]\n",
    "    group_target = target_info[2]\n",
    "    \n",
    "    if group_source == group_target:\n",
    "        group_similarity_test.append(1)\n",
    "    else:\n",
    "        group_similarity_test.append(0)\n",
    "        \n",
    "    # number of reviews similarity\n",
    "    review_source = source_info[4]\n",
    "    review_target = target_info[4]\n",
    "    \n",
    "    reviews_num_similarity_test.append(np.absolute(int(review_source)-int(review_target)))\n",
    "    \n",
    "    # same number of words in detailed category\n",
    "    category_source = source_info[6+int(review_source)*4:]\n",
    "    category_target = target_info[6+int(review_target)*4:]\n",
    "    \n",
    "    category_similarity_test.append(len(set(category_source).intersection(set(category_target))))\n",
    "    \n",
    "    # rating similarity\n",
    "    rating_source = source_info[5]\n",
    "    rating_target = target_info[5]\n",
    "    \n",
    "    rating_similarity_test.append(np.absolute(float(rating_source)-float(rating_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"testing examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum of nodes' degrees \n",
    "node_degree_test = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "#for i in range(1):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    #add node degree\n",
    "    source_degree = G.degree(source)\n",
    "    target_degree = G.degree(target)\n",
    "    if isinstance(source_degree,dict):\n",
    "        source_degree = 0\n",
    "    if isinstance(target_degree,dict):\n",
    "        target_degree = 0\n",
    "    node_degree_test.append(source_degree+target_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distance between two nodes in graph\n",
    "dist_node_test = []\n",
    "\n",
    "counter = 0\n",
    "node_set = G.nodes()\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    if (source in node_set) and (target in node_set) and nx.has_path(G,source,target):\n",
    "        dist_node_test.append(-nx.shortest_path_length(G,source,target))\n",
    "    else:\n",
    "        dist_node_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of common neighbors\n",
    "common_neighbors_test = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    #add node degree\n",
    "    common_neighbors_test.append(len(list(nx.common_neighbors(G,source,target))))\n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# difference between betweenness-centralities of target and source\n",
    "# idea comes from https://www.researchgate.net/publication/220432611_Link_Prediction_in_Citation_Networks, page 80-81\n",
    "btw_centralities_test = []\n",
    "\n",
    "counter = 0\n",
    "node_set = G.nodes()\n",
    "\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    if (source in all_btw_centralities) and (target in all_btw_centralities):\n",
    "        btw_centralities_test.append(all_btw_centralities[target] - all_btw_centralities[source])\n",
    "    else:\n",
    "        btw_centralities_test.append(0.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array([title_similarity_test, group_similarity_test, category_similarity_test,reviews_num_similarity_test,rating_similarity_test,node_degree_test,dist_node_test,common_neighbors_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction SVM\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "#predictions = zip(range(len(testing_set)), predictions_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction Linear\n",
    "result = [int(t[2]) for t in testing_set]\n",
    "predictions_linear = []\n",
    "for i in xrange(len(testing_features)):\n",
    "#for i in range(50):\n",
    "    r = inner(theta,testing_features[i])\n",
    "    #if r >= 0:\n",
    "    #    predictions_linear.append(1)\n",
    "    #else:\n",
    "    #    predictions_linear.append(0)\n",
    "    predictions_linear.append(r)\n",
    "predictions = zip(result, predictions_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "result = [int(t[2]) for t in testing_set]\n",
    "compare = zip(result,predictions_SVM)\n",
    "accuracy = 0\n",
    "for c in compare:\n",
    "    if c[0] == c[1]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy*1.0 / len(compare)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n"
     ]
    }
   ],
   "source": [
    "result = [int(t[2]) for t in testing_set]\n",
    "compare = zip(result,predictions_SVM)\n",
    "accuracy = 0\n",
    "for c in compare:\n",
    "    if c[0] == c[1] and c[0] == 1:\n",
    "        accuracy += 1\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5502\n"
     ]
    }
   ],
   "source": [
    "count_p1 = 0\n",
    "for p in predictions_SVM:\n",
    "    if p == 1:\n",
    "        count_p1 += 1\n",
    "print(count_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for p in predictions:\n",
    "    if p[0] == 1 and p[1] >= 0:\n",
    "        accuracy += 1\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "\n",
    "with open(\"improved_predictions.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(['id','category'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
