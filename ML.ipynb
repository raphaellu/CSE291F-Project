{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'testing_set.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-13170d79279c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing_set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtesting_set\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'testing_set.txt'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#import igraph\n",
    "import csv\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner(X,Y):\n",
    "    s = 0\n",
    "    for i in range(0,len(X)):\n",
    "        s = s + X[i]*Y[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04330019]\n"
     ]
    }
   ],
   "source": [
    "source_tm = features_TFIDF[6].toarray().T\n",
    "target_tm = features_TFIDF[5].toarray().T\n",
    "cos_similarity = inner(source_tm,target_tm)\\\n",
    "    / (np.linalg.norm(np.array(source_tm))\\\n",
    "       *np.linalg.norm(np.array(target_tm)))\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.05)))\n",
    "training_set_reduced_total = [training_set[i] for i in to_keep]\n",
    "training_set_reduced = training_set_reduced_total[:len(training_set_reduced_total)/2]\n",
    "tmp_test = training_set_reduced_total[len(training_set_reduced_total)/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tmp test examples processsed\n",
      "1001 tmp test examples processsed\n",
      "2001 tmp test examples processsed\n",
      "3001 tmp test examples processsed\n",
      "4001 tmp test examples processsed\n",
      "5001 tmp test examples processsed\n",
      "6001 tmp test examples processsed\n",
      "7001 tmp test examples processsed\n",
      "8001 tmp test examples processsed\n",
      "9001 tmp test examples processsed\n",
      "10001 tmp test examples processsed\n",
      "11001 tmp test examples processsed\n",
      "12001 tmp test examples processsed\n",
      "13001 tmp test examples processsed\n",
      "14001 tmp test examples processsed\n",
      "15001 tmp test examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of overlapping words in title\n",
    "overlap_title_tmp_test = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff_tmp_test = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth_tmp_test = []\n",
    "\n",
    "text_mining_tmp_test = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(tmp_test)):\n",
    "    source = tmp_test[i][0]\n",
    "    target = tmp_test[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_tmp_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_tmp_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_tmp_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    #add text mining\n",
    "    source_tm = source_info[6]\n",
    "    target_tm = target_info[6]\n",
    "    \n",
    "    cos_similarity = np.dot(source_tm,target_tm)\\\n",
    "    / (np.linalg.norm(np.array(source_tm))\\\n",
    "       *np.linalg.norm(np.array(target_tm)))\n",
    "    text_mining_tmp_test.append(cos_similarity)\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"tmp test examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text mining technique\n",
    "from collections import defaultdict\n",
    "import string\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "for d in node_info:    \n",
    "    r = ''.join([c for c in d[5].lower() if not c in punctuation])\n",
    "    l = r.split()\n",
    "    for i in range(1, len(l)):\n",
    "        if (l[i-1] not in stpwds) and (l[i] not in stpwds):\n",
    "            w = l[i-1] + ' ' + l[i]\n",
    "            wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum[5].lower() if not c in punctuation])\n",
    "  l = r.split()\n",
    "  for i in range(1, len(l)):\n",
    "        w = l[i-1] + ' ' + l[i]\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "\n",
    "X_train = [feature(d) for d in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(node_info)):\n",
    "    node_info[i].append(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of overlapping words in title\n",
    "overlap_title_tm = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff_tm = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth_tm = []\n",
    "\n",
    "# text_mining vector\n",
    "text_mining = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_tm.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_tm.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_tm.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    #add text mining\n",
    "    source_tm = features_TFIDF[index_source].toarray().T\n",
    "    target_tm = features_TFIDF[index_target].toarray().T\n",
    "    \n",
    "    cos_similarity = inner(source_tm,target_tm)\\\n",
    "    / (np.linalg.norm(np.array(source_tm))\\\n",
    "       *np.linalg.norm(np.array(target_tm)))\n",
    "    text_mining.append(cos_similarity)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27770"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_info[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "#training_features = np.array([overlap_title, temp_diff, comm_auth]).T\n",
    "training_features = np.array([overlap_title_tm, temp_diff_tm, comm_auth_tm, text_mining]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57103718, -0.87475768, -0.23554775, -0.47786327])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random prediction\n",
    "#predictions_random = np.random.choice([0, 1], size=len(tmp_test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0697167   0.11627358  0.00881498  0.22260221]\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "theta = np.linalg.lstsq(training_features, labels_array)[0]\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ba7c736769d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize basic SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n",
      "5001 testing examples processsed\n",
      "6001 testing examples processsed\n",
      "7001 testing examples processsed\n",
      "8001 testing examples processsed\n",
      "9001 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "11001 testing examples processsed\n",
      "12001 testing examples processsed\n",
      "13001 testing examples processsed\n",
      "14001 testing examples processsed\n",
      "15001 testing examples processsed\n",
      "16001 testing examples processsed\n",
      "17001 testing examples processsed\n",
      "18001 testing examples processsed\n",
      "19001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "21001 testing examples processsed\n",
      "22001 testing examples processsed\n",
      "23001 testing examples processsed\n",
      "24001 testing examples processsed\n",
      "25001 testing examples processsed\n",
      "26001 testing examples processsed\n",
      "27001 testing examples processsed\n",
      "28001 testing examples processsed\n",
      "29001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "31001 testing examples processsed\n",
      "32001 testing examples processsed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a8b2376e340d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtesting_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    127\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    128\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "overlap_title_test = []\n",
    "temp_diff_test = []\n",
    "comm_auth_test = []\n",
    "text_mining = []\n",
    "   \n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    #add text mining\n",
    "    source_tm = features_TFIDF[index_source].toarray().T\n",
    "    target_tm = features_TFIDF[index_target].toarray().T\n",
    "    \n",
    "    cos_similarity = inner(source_tm,target_tm)\\\n",
    "    / (np.linalg.norm(np.array(source_tm))\\\n",
    "       *np.linalg.norm(np.array(target_tm)))\n",
    "    text_mining.append(cos_similarity)\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"testing examples processsed\"\n",
    "        \n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "#testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test]).T\n",
    "#for text mining\n",
    "testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test,text_mining]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions_SVM = list(classifier.predict(testing_features))\n",
    "predictions = zip(range(len(testing_set)), predictions_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_linear = []\n",
    "for i in xrange(len(testing_set)):\n",
    "    r = inner(theta,testing_features[i])\n",
    "    if r >= 0:\n",
    "        predictions_linear.append(1)\n",
    "    else:\n",
    "        predictions_linear.append(0)\n",
    "predictions = zip(range(len(testing_set)), predictions_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_linear = []\n",
    "for i in xrange(len(testing_set)):\n",
    "    r = inner(theta,testing_features[i])\n",
    "    predictions_linear.append(r)\n",
    "predictions = zip(range(len(testing_set)), predictions_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "\n",
    "with open(\"improved_predictions.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(['id','category'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_test_features = np.array([overlap_title_tmp_test,temp_diff_tmp_test,comm_auth_tmp_test,text_mining_tmp_test]).T\n",
    "# scale\n",
    "tmp_test_features = preprocessing.scale(tmp_test_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "tmp_test_labels = [int(element[2]) for element in training_set_reduced]\n",
    "tmp_test_labels = list(labels)\n",
    "tmp_test_labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_linear = []\n",
    "for i in xrange(len(tmp_test_labels_array)):\n",
    "    r = inner(theta,tmp_test_features[i])\n",
    "    if r >= 0.5:\n",
    "        predictions_linear.append(1)\n",
    "    else:\n",
    "        predictions_linear.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_SVM = np.array(list(classifier.predict(tmp_test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543995321029\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "for i in xrange(len(predictions_linear)):\n",
    "    MSE += (predictions_linear[i]-tmp_test_labels_array[i])**2\n",
    "MSE = MSE*1.0 / len(predictions_linear)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
