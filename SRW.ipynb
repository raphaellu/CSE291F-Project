{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs, fmin_l_bfgs_b\n",
    "\n",
    "# ***function: iterPageRank\n",
    "# this function takes initial node probabilities and a \n",
    "# transition matrix then use power iteration to find \n",
    "# the PageRank of nodes\n",
    "def iterPageRank(pp, trans):\n",
    "    ppnew = np.dot(pp, trans)\n",
    "    while not(np.allclose(pp, ppnew)):\n",
    "        pp = ppnew\n",
    "        ppnew = np.dot(pp, trans)\n",
    "    return ppnew\n",
    "\n",
    "\n",
    "# ***function: genCopyGraph\n",
    "# generate an undirected random graph with copyig model\n",
    "# the generated graph has the property of preferencial attachment\n",
    "# an edge list is returned\n",
    "def genCopyGraph(nnodes, alpha):\n",
    "    # since the copy model starts with a triad, the input number of \n",
    "    # nodes should be larger than 3\n",
    "    if nnodes <= 3:\n",
    "        print \"Number of nodes should be larger than 3...\"\n",
    "        return\n",
    "    # inital setting with a triad\n",
    "    degrees = np.repeat(2, 3)\n",
    "    edges = [(0, 1), (0, 2), (1, 2)]\n",
    "    # growing the graph node by node\n",
    "    for i in range(3, nnodes):\n",
    "        # add three edges for the new node\n",
    "        for nedge in range(3):\n",
    "            if np.random.rand() < alpha:\n",
    "                # uniformly choose node to connect\n",
    "                tar = np.random.choice(i, 1)[0]\n",
    "                while (tar, i) in edges:\n",
    "                    tar = np.random.choice(i, 1)[0]\n",
    "                edges.append((tar, i))\n",
    "                degrees[tar] += 1\n",
    "            else:\n",
    "                # select target to connect according to degree\n",
    "                sDeg = sum(degrees)\n",
    "                tar = -1\n",
    "                firstRound = True\n",
    "                while (tar, i) in edges or firstRound:\n",
    "                    firstRound = False\n",
    "                    randPick = np.random.randint(1, sDeg+1)\n",
    "                    accu = 0\n",
    "                    for j in range(len(degrees)):\n",
    "                        accu += degrees[j]\n",
    "                        if randPick <= accu:\n",
    "                            tar = j\n",
    "                            break\n",
    "                        \n",
    "                edges.append((tar, i))\n",
    "                degrees[tar] += 1\n",
    "        \n",
    "        degrees = np.append(degrees, 3)\n",
    "    return [edges, degrees]\n",
    "\n",
    "\n",
    "# ***function: calStrength\n",
    "# return edge strength calculated by logistic function\n",
    "# the inputs are two vectors, features and the parameters\n",
    "def calStrength(features, beta):\n",
    "    #return np.exp(np.dot(features, beta))\n",
    "\t# use logistic strength function to prevent potential overflow or under flow\n",
    "\t# of floating point numbers\n",
    "\treturn  1.0 / (1+ np.exp(-1 * np.dot(features, beta) ))\n",
    "\n",
    "\n",
    "# ***function: strengthDiff\n",
    "# calculate an returns the gradient of strength functioin with \n",
    "# respect to beta, the returned value is a vector\n",
    "def strengthDiff(features, beta):\n",
    "    # return a vector of gradient of strength\n",
    "    diff = []\n",
    "    denom = calStrength(features, beta) ** 2\n",
    "    numer_exp = np.exp(-1 * np.dot(features, beta))\n",
    "    for k in range(len(beta)):\n",
    "        diff.append(features[k] * numer_exp * denom)\n",
    "    return diff\n",
    "\n",
    "\n",
    "# ***function: genTrans\n",
    "# this function takes in a graph (edge list and number of nodes), \n",
    "# node features, source node, and alpha/beta parameters to generate\n",
    "# a random walk transition matrix.\n",
    "# transition probabilities are determined by edge strength\n",
    "# beta is the parameter in the edge strength function\n",
    "# alpha is the teleportation rate back to the source node\n",
    "def genTrans(nnodes, g, features, s, alpha, beta):\n",
    "    # feature is supplied in per-edge manner\n",
    "    # the transition matrix is created with teleportation\n",
    "    trans = np.zeros((nnodes, nnodes))\n",
    "    for i in range(len(g)):\n",
    "        #strength = calStrength(np.asarray(features[g[i][0],])*np.asarray(features[g[i][1],])\n",
    "        #, beta)\n",
    "        strength = calStrength(features[g[i][0]][g[i][1]], beta)\n",
    "        trans[g[i][0], g[i][1]] = strength\n",
    "        trans[g[i][1], g[i][0]] = strength\n",
    "    \n",
    "    # normalize the transition matrix\n",
    "    for i in range(nnodes):\n",
    "        tempSum = sum(trans[i,])\n",
    "        if tempSum > 0:\n",
    "            trans[i,] = map(lambda x: x/tempSum, trans[i, ])\n",
    "    \n",
    "    # create a list of transition matrices for a set of sources\n",
    "    trans_multi = []    \n",
    "    \n",
    "    for si in range(len(s)):\n",
    "    # create the one matrix\n",
    "        one = np.zeros((nnodes, nnodes))\n",
    "        for i in range(nnodes):\n",
    "            one[i, s[si]] = 1\n",
    "            \n",
    "        # combine the regular transition matrix and the one matrix\n",
    "        trans_multi.append((1-alpha)*trans + alpha*one) \n",
    "    \n",
    "    return trans_multi\n",
    "\n",
    "# ***function: genTrans_plain\n",
    "# this function construct transition matrix for random walk\n",
    "# with unweighted edge strenght, i.e. each eade has strength 1\n",
    "def genTrans_plain(nnodes, g, s, alpha):\n",
    "    trans = np.zeros((nnodes, nnodes))\n",
    "    for i in range(len(g)):\n",
    "        trans[g[i][0], g[i][1]] = 1\n",
    "        trans[g[i][1], g[i][0]] = 1\n",
    "    \n",
    "    # normalize the transition matrix\n",
    "    for i in range(nnodes):\n",
    "        tempSum = sum(trans[i,])\n",
    "        if tempSum > 0:\n",
    "            trans[i,] = map(lambda x: x/tempSum, trans[i, ])\n",
    "    \n",
    "    # create a list of transition matrices for a set of sources\n",
    "    trans_multi = []    \n",
    "    \n",
    "    for si in range(len(s)):\n",
    "        # create the one matrix\n",
    "        one = np.zeros((nnodes, nnodes))\n",
    "        for i in range(nnodes):\n",
    "            one[i, s[si]] = 1\n",
    "            \n",
    "        # combine the regular transition matrix and the one matrix\n",
    "        trans_multi.append((1-alpha)*trans + alpha*one)\n",
    "    \n",
    "    return trans_multi\n",
    "\n",
    "\n",
    "# ***function: genTrans_tele\n",
    "# this function takes in a set of teleport nodes and compute the \n",
    "# transition matrix accordingly. the difference between genTrans function\n",
    "# is that genTrans produces transition matrices for single source nodes,\n",
    "# while genTrans_tele produces transition matrices for a set of teleport\n",
    "# nodes.\n",
    "def genTrans_tele(nnodes, g, features, tele, alpha, beta):\n",
    "    # feature is supplied in per-edge manner\n",
    "    # the transition matrix is created with teleportation\n",
    "    trans = np.zeros((nnodes, nnodes))\n",
    "    for i in range(len(g)):\n",
    "        strength = calStrength(features[g[i][0]][g[i][1]], beta)\n",
    "        trans[g[i][0], g[i][1]] = strength\n",
    "        trans[g[i][1], g[i][0]] = strength\n",
    "    \n",
    "    # normalize the transition matrix\n",
    "    for i in range(nnodes):\n",
    "        tempSum = sum(trans[i,])\n",
    "        if tempSum > 0:\n",
    "            trans[i,] = map(lambda x: x/tempSum, trans[i, ])\n",
    "    \n",
    "    # create a list of transition matrices for a set of teleport sets\n",
    "    trans_multi = []\n",
    "    \n",
    "    for t in range(len(tele)):\n",
    "        teleSize = len(tele[t])\n",
    "        one = np.zeros((nnodes, nnodes))\n",
    "        if teleSize > 0:\n",
    "            for i in range(nnodes):\n",
    "                for j in range(teleSize):\n",
    "                    one[i, tele[t][j]] = 1.0/teleSize\n",
    "            # the calculated transition matrix with teleportation\n",
    "            trans_multi.append((1-alpha)*trans + alpha*one)\n",
    "        else:\n",
    "            # transition matrix without transporation since the \n",
    "            # input teleport set is empty\n",
    "            trans_multi.append(trans)\n",
    "    \n",
    "    return trans_multi\n",
    "\n",
    "# ***function: genFeatures\n",
    "# input features are in edge list form, this function transfer the features into\n",
    "# matrix-style, with each element in the matrix a the feature vecotr of \n",
    "# corresponding edge, the element is [] if the particular edge does not exist\n",
    "def genFeatures(nnodes, g, features):\n",
    "    # very elemnt in the array is a list\n",
    "    fea = [[ [] for x in range(nnodes) ] for x in range(nnodes) ]\n",
    "    # create a feature matrix\n",
    "    for i in range(len(g)):\n",
    "        fea[g[i][0]][g[i][1]] = features[i]\n",
    "        fea[g[i][1]][g[i][0]] = features[i]\n",
    "    \n",
    "    return fea\n",
    "\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "## Below are the functions for learning process\n",
    "\n",
    "# ***function: iterPageDiff\n",
    "# this function use power-iteration-like method to return the gradient of \n",
    "# Supervised Random Walk pagerank scores\n",
    "def iterPageDiff(pdiff, p, trans, transdiff):\n",
    "    pdiffnew = np.dot(pdiff, trans) + np.dot(p, transdiff)\n",
    "    while not(np.allclose(pdiff, pdiffnew)):\n",
    "        pdiff = pdiffnew\n",
    "        pdiffnew = np.dot(pdiff, trans) + np.dot(p, transdiff)\n",
    "    return pdiffnew[0]\n",
    "\n",
    "\n",
    "#########################################\n",
    "### this function is not used anymore ###\n",
    "#########################################\n",
    "# ***function: diffQelem\n",
    "# this function is called by diffQ, return the (i, j)-th element of the \n",
    "# derivative of transition matrix with respect to k-th element of beta\n",
    "def diffQelem(features, beta, trans_p, alpha, row, col, k):\n",
    "    # calculates the element value of transition matrix's differentiation\n",
    "    # first calculate the denominator part    \n",
    "    denom = 0\n",
    "    xdenom = 0\n",
    "    for j in range(int(np.shape(trans_p)[1])):\n",
    "        if trans_p[row, j] > 0:\n",
    "            # should check on the original version of transition matrix, \n",
    "            # because teleportation does not contribute to gradient\n",
    "            #temp = calStrength(np.asarray(features[row,])*np.asarray(features[j,])\n",
    "            #, beta)\n",
    "            temp = calStrength(features[row][j], beta)\n",
    "            denom += temp\n",
    "            #xdenom += (np.asarray(features[row,])*np.asarray(features[j,]))[k] * temp\n",
    "            xdenom += features[row][j][k] * temp\n",
    "    #curFeat = np.asarray(features[row,])*np.asarray(features[col,])\n",
    "    curFeat = features[row][col]\n",
    "    strength = calStrength(curFeat, beta)\n",
    "    \n",
    "    elem = (1-alpha)*(curFeat[k]*strength*denom - strength*xdenom) / (denom**2)\n",
    "    \n",
    "    return elem\n",
    "\n",
    "\n",
    "# ***function: diffQ\n",
    "# given a Supervised Random Walk transition matrix, return the derivative of \n",
    "# transition matrix with respect to the k-th element in parameter beta\n",
    "def diffQ(features, beta, trans_p, alpha):\n",
    "    \n",
    "    nnodes = int(np.shape(trans_p)[0])\n",
    "\t\n",
    "    # first compute the (unnormalized) edge strength matrix and the gradient matrix\n",
    "    sMat = np.zeros((nnodes, nnodes))\n",
    "    for i in range(int(np.shape(trans_p)[0])):\n",
    "        for j in range(i, int(np.shape(trans_p)[1])):\n",
    "            if trans_p[i, j] > 0:\n",
    "                strength = calStrength(features[i][j], beta)\n",
    "                sMat[i, j] = strength\n",
    "                sMat[j, i] = strength\n",
    "    \n",
    "    # gradQ is the gradient of strength matrix\n",
    "    # a list of matrices is computed, with the k-th item in the list be the \n",
    "    # derivative of strength matrix with respect to the k-th element in beta\n",
    "    # vecotr\n",
    "    gradS = []\n",
    "    for i in range(len(beta)):\n",
    "        gradS.append(np.zeros((nnodes, nnodes)))    \n",
    "    for i in range(int(np.shape(trans_p)[0])):\n",
    "        for j in range(int(np.shape(trans_p)[1])):\n",
    "            if trans_p[i, j] > 0:\n",
    "                gradTemp = strengthDiff(features[i][j], beta)\n",
    "                for k in range(len(beta)):\n",
    "                    gradS[k][i, j] = gradTemp[k]\n",
    "    \n",
    "    \n",
    "    # compute the gradient of transition matrix\n",
    "    # a list of matrices is computed, with k-th element in the list be \n",
    "    # the derivative of transition matrix with respect to the k-th \n",
    "    # element in parameter vecotor beta\n",
    "    qp = []\n",
    "    for i in range(len(beta)):\n",
    "        qp.append(np.zeros((nnodes, nnodes)))\n",
    "    \n",
    "    for i in range(int(np.shape(trans_p)[0])):\n",
    "        # for each row in the gradient matrix, some common factors can be \n",
    "        # computed first\n",
    "        sumStrength = 0\n",
    "        sumDiff = [0] * len(beta)\n",
    "        for j in range(int(np.shape(trans_p)[1])):\n",
    "            if trans_p[i, j] > 0:\n",
    "                sumStrength += sMat[i, j]\n",
    "                for k in range(len(beta)):\n",
    "                    sumDiff[k] += gradS[k][i, j]\n",
    "        # individual entries can then be computed\n",
    "        for j in range(int(np.shape(trans_p)[1])):\n",
    "            if trans_p[i, j] > 0:\n",
    "                for k in range(len(beta)):\n",
    "                    qp[k][i, j] = (sumStrength ** -2)*( gradS[k][i, j]*sumStrength -\n",
    "                    sMat[i, j]*sumDiff[k])*(1 - alpha)\n",
    "        \n",
    "    return qp\n",
    "\n",
    "\n",
    "# ***function: costFunc\n",
    "# this is the cost function in learning process\n",
    "# given the page rank socres of two nodes, one in future link set\n",
    "# one in no-link set, and an offset parameter, the cost function value is \n",
    "# returned. (return value is a scalar)\n",
    "def costFunc(pl, pd, offset):\n",
    "    #return (max(0, pl - pd + offset))**2\n",
    "    return 1.0 / (1 + np.exp(-1.0 * (pl - pd)/offset))\n",
    "\n",
    "\n",
    "# ***function: costDiff\n",
    "# this is the gradient of cost function\n",
    "# given the page rank socres of two nodes, one in future link set\n",
    "# one in no-link set, and an offset parameter, the gradient of cost function \n",
    "# is returned. (return value is a scalar)\n",
    "def costDiff(pl, pd, offset):\n",
    "    #return 2.0*(max(0, pl - pd + offset))\n",
    "    return (1.0 / offset) * np.exp(-1.0 * (pl - pd)/offset) * (costFunc(pl, pd, offset) ** 2)\n",
    "\n",
    "\n",
    "# ***function: minObj\n",
    "# this is the object function to be minimized in the learning process\n",
    "# the future link set and no-link from training set should be given, also, \n",
    "# parameters in the learning process and the graph of training set should be \n",
    "# given.\n",
    "# Supervised Random Walk pagerank scores are calculated for each node based on \n",
    "# input training graph and features, then cost function value is calculated \n",
    "# according to the derived pagerank scores.\n",
    "# (return value is a scalar)\n",
    "def minObj(Dset, Lset, offset, lam, nnodes, g, features, source, alpha, beta):\n",
    "    # calculate PageRank according to features and beta values\n",
    "    \n",
    "    # transform input features into matrix form\n",
    "    features_m = genFeatures(nnodes, g, features)\n",
    "    \n",
    "    # compute transition matrices for sources\n",
    "    trans = genTrans(nnodes, g, features_m, source, alpha, beta)\n",
    "    \n",
    "    # cost value    \n",
    "    cost = 0\n",
    "    # calculate cost function for every selected sources nodes\n",
    "    for i in range(len(source)):\n",
    "        pp = np.repeat(1.0/nnodes, nnodes)\n",
    "        pgrank = iterPageRank(pp, trans[i])\n",
    "        \n",
    "        # compute cost from the generated PageRank value    \n",
    "        for d in Dset[i]:\n",
    "            for l in Lset[i]:\n",
    "                cost += costFunc(pgrank[l], pgrank[d], offset)\n",
    "    \n",
    "    penalty = lam * np.dot(beta, beta)\n",
    "    \n",
    "    return (cost + penalty)\n",
    "\n",
    "\n",
    "# ***function: objDiff\n",
    "# this is the gradient of the object function\n",
    "# given training graph, training sets and parameters, gradient of the object \n",
    "# function is returned. This is required in gradient descent and BFGS optimization\n",
    "# processes.\n",
    "# Supervised Random Walk pagerank is calculated then served as a basis to compute\n",
    "# the gradient of pagerank scores. Gradient of pagerank scores are derived by \n",
    "# power-iteration-like method.\n",
    "# (return value is a vector with the dimension of parameter beta)\n",
    "def objDiff(Dset, Lset, offset, lam, nnodes, g, features, source, alpha, beta):\n",
    "    diffVec = [0] * len(beta)\n",
    "    # calculate PageRank according to features and beta values\n",
    "    \n",
    "    # transform input features into matrix form\n",
    "    features_m = genFeatures(nnodes, g, features)        \n",
    "    \n",
    "    ###########################################################\n",
    "    ### trans_p and transDiff are independent of source node ##\n",
    "    ###########################################################\n",
    "    # trans_p is the original transition matrix \n",
    "    # (without teleportation and varying strength)\n",
    "    # this is used to calculate gradient of transition matrix\n",
    "    trans_p = genTrans_plain(nnodes, g, [0], 0)[0]\n",
    "        \n",
    "    # a list of matrices is returned by diffQ function\n",
    "    transDiff = diffQ(features_m, beta, trans_p, alpha)\n",
    "    ###########################################################\n",
    "    ###########################################################\n",
    "    \n",
    "    # compute transition matrices for sources\n",
    "    trans = genTrans(nnodes, g, features_m, source, alpha, beta)\n",
    "    \n",
    "    # calculate gradient for every selected sources nodes\n",
    "    for i in range(len(source)):\n",
    "    \n",
    "        pp = np.repeat(1.0/nnodes, nnodes)\n",
    "        pgrank = iterPageRank(pp, trans[i])\n",
    "        \n",
    "        for k in range(len(beta)):\n",
    "            tempObjDiff = 0\n",
    "            pDiff = np.zeros((1, nnodes))\n",
    "            pDiff = iterPageDiff(pDiff, pgrank, trans[i], transDiff[k])\n",
    "            for d in Dset[i]:\n",
    "                for l in Lset[i]:\n",
    "                    tempObjDiff += costDiff(pgrank[l], pgrank[d], offset)*(pDiff[l] - pDiff[d])\n",
    "            # penalty term\n",
    "            #tempObjDiff += 2.0 * lam * beta[k]\n",
    "            \n",
    "            #diffVec.append(tempObjDiff)\n",
    "            diffVec[k] += tempObjDiff\n",
    "    \n",
    "    # penalty term\n",
    "    for k in range(len(beta)):\n",
    "        diffVec[k] += 2.0 * lam * beta[k]\n",
    "    \n",
    "    return np.asarray(diffVec)\n",
    "\n",
    "        \n",
    "# ***function: trainModel\n",
    "# users call this function to train beta parameter of Supervised Random Walk algorithm\n",
    "# a training set and training graph must be specified as well as the parameters for the \n",
    "# learning process. Also, initial guess of beta parameter shall be given\n",
    "# scipy's L-BFGS-B optimizer is called to iteratively optimize the object function,\n",
    "# object function and the gradient of cost function is the main input to BFGS\n",
    "# optimizer\n",
    "def trainModel(Dset, Lset, offset, lam, nnodes, g, features, source, alpha, beta_init):\n",
    "    #beta_Opt = fmin_bfgs(functools.partial(minObj, Dset, Lset, 0, 0, nnodes, g, features, \n",
    "    #                        source, alpha), beta_init, fprime = functools.partial(objDiff, \n",
    "    #                        Dset, Lset, 0, 0, nnodes, g, features, source, alpha))\n",
    "    \n",
    "    beta_Opt = fmin_l_bfgs_b(functools.partial(minObj, Dset, Lset, offset, lam, nnodes, g, features, \n",
    "                            source, alpha), beta_init, fprime = functools.partial(objDiff, \n",
    "                            Dset, Lset, offset, lam, nnodes, g, features, source, alpha))\n",
    "    \n",
    "    return beta_Opt\n",
    "\n",
    "\n",
    "############################################\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Forming training set...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print \"Reading data...\"\n",
    "\n",
    "# load the sanpshots of 6-30 and 12-31,\n",
    "# 6-30 is a graph used as a basis of the learning process\n",
    "# 12-31 provides both training data and test data\n",
    "fp = open('amazon-meta_item_item_1.txt', 'r')\n",
    "fp_end = open('amazon-meta_item_item_2.txt', 'r')\n",
    "\n",
    "# determine random range\n",
    "with open(\"amazon-meta_filtered_ground_truth.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_list  = list(reader)\n",
    "f.close()\n",
    "\n",
    "nnodes = len(testing_list)\n",
    "degrees = [0] * nnodes\n",
    "edges = []\n",
    "edges_end = []\n",
    "features = [[], []]\n",
    "features_end = [[], []]\n",
    "for line in fp:\n",
    "    temp = line.strip().split(',')\n",
    "    edges.append((int(temp[0]), int(temp[1])))\n",
    "    features[0].append(float(temp[2]))\n",
    "    features[1].append(float(temp[3]))\n",
    "    degrees[int(temp[0])] += 1\n",
    "    degrees[int(temp[1])] += 1\n",
    "\n",
    "for line in fp_end:\n",
    "    temp = line.strip().split(',')\n",
    "    edges_end.append((int(temp[0]), int(temp[1])))\n",
    "    features_end[0].append(float(temp[2]))\n",
    "    features_end[1].append(float(temp[3]))\n",
    "\n",
    "fp.close()\n",
    "fp_end.close()\n",
    "\n",
    "# normalize features\n",
    "u0 = np.mean(features[0])\n",
    "u1 = np.mean(features[1])\n",
    "s0 = np.std(features[0])\n",
    "s1 = np.std(features[1])\n",
    "\n",
    "features[0] = map(lambda x: (x-u0)/s0, features[0])\n",
    "features[1] = map(lambda x: (x-u1)/s1, features[1])\n",
    "\n",
    "edge_feature = []\n",
    "# features are formed with intercept term\n",
    "for i in range(len(edges)):\n",
    "    edge_feature.append([features[0][i], features[1][i]])\n",
    "\n",
    "\n",
    "#######################################\n",
    "#### Training set formation ###########\n",
    "#######################################\n",
    "\n",
    "print \"Forming training set...\"\n",
    "\n",
    "# compute the candidate set for future links according to source node\n",
    "# train model with a set of source nodes\n",
    "elig_source = []\n",
    "for i in range(len(degrees)):\n",
    "    if degrees[i] > 0:\n",
    "        elig_source.append(i)\n",
    "\n",
    "# pick nodes with number of future links larger than theshold\n",
    "# these nodes then served as either training node or test node\n",
    "Dsize_cut = 0\n",
    "\n",
    "D_source = []\n",
    "Dset_all = []\n",
    "Lset_all = []\n",
    "for i in range(len(elig_source)):\n",
    "    sNeighbor = []\n",
    "    for e in edges:\n",
    "        if e[0] == elig_source[i]:\n",
    "            sNeighbor.append(e[1])\n",
    "        elif e[1] == elig_source[i]:\n",
    "            sNeighbor.append(e[0])\n",
    "    candidates = list(set(list(range(nnodes))) - set([elig_source[i]]) - set(sNeighbor))\n",
    "    \n",
    "    sNeighbor_end = []\n",
    "    for e in edges_end:\n",
    "        if e[0] == elig_source[i]:\n",
    "            sNeighbor_end.append(e[1])\n",
    "        elif e[1] == elig_source[i]:\n",
    "            sNeighbor_end.append(e[0])\n",
    "    tempDset = list(set(sNeighbor_end) - set(sNeighbor))\n",
    "    if len(tempDset) >= Dsize_cut:\n",
    "        tempLset = list(set(candidates) - set(tempDset))\n",
    "        Dset_all.append(tempDset)\n",
    "        Lset_all.append(tempLset)\n",
    "        D_source.append(elig_source[i])\n",
    "\n",
    "# randomly pick nodes with current degree > 0 and number of future \n",
    "# links >= Dsize_cut as the training set\n",
    "trainSize = 100\n",
    "testSize = 50\n",
    "# this index is the index of source nodes in D_source list\n",
    "source_index = np.random.choice(list(range(len(D_source))), \n",
    "                                size=trainSize, replace=False)\n",
    "source = []\n",
    "Dset = []\n",
    "Lset = []\n",
    "for i in source_index:\n",
    "    source.append(D_source[i])\n",
    "    Dset.append(Dset_all[i])\n",
    "    Lset.append(Lset_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Training source set:\n",
      "[1051, 367, 147, 123, 1106, 317, 779, 1306, 167, 658, 7, 1304, 69, 512, 722, 267, 1057, 1116, 853, 1147, 107, 1388, 1274, 1095, 659, 217, 636, 1385, 511, 1242, 180, 1040, 53, 733, 1445, 417, 92, 119, 185, 754, 1325, 987, 1496, 886, 2, 1403, 369, 504, 1165, 1172, 354, 81, 1369, 1006, 817, 601, 1508, 924, 806, 398, 753, 1367, 1312, 232, 965, 226, 1107, 1402, 1525, 1259, 347, 1389, 399, 1183, 29, 1269, 892, 1500, 1268, 550, 640, 1223, 200, 1284, 198, 786, 51, 542, 1406, 1044, 685, 1251, 1247, 1541, 266, 154, 982, 452, 246, 293]\n",
      "\n",
      "Trained model parameters:\n",
      "(array([-0.1112265 ,  0.84796109]), 13568.613994285204, {'warnflag': 0, 'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'grad': array([ -3.61887626e-04,   8.77368985e-05]), 'nit': 5, 'funcalls': 6})\n",
      "Evaluating model performance...\n",
      "\n",
      "SRW performance:  0.192504258944\n"
     ]
    }
   ],
   "source": [
    "#test_index = np.random.choice(list(set(list(range(len(D_source)))) - \n",
    "#set(source_index)), size=testSize, replace=False)\n",
    "\n",
    "# enlarge testing set\n",
    "test_index = list(set(list(range(len(D_source)))) - set(source_index))\n",
    "\n",
    "testSet = []\n",
    "Dset_test = []\n",
    "Lset_test = []\n",
    "candidates_test = []\n",
    "\n",
    "\n",
    "# original code\n",
    "\n",
    "for i in test_index:\n",
    "    testSet.append(D_source[i])\n",
    "    Dset_test.append(Dset_all[i])\n",
    "    Lset_test.append(Lset_all[i])\n",
    "    candidates_test.append(Dset_all[i] + Lset_all[i])\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "#### Model training phase #############\n",
    "#######################################\n",
    "\n",
    "print \"Training model...\"\n",
    "\n",
    "# set up parameters\n",
    "lam = 50\n",
    "offset = 0.01\n",
    "alpha = 0.3\n",
    "beta_init = [0.5, 0.5]\n",
    "\n",
    "#ff = genFeatures(nnodes, edges, edge_feature)\n",
    "#trans_p = genTrans_plain(nnodes, edges, 0, 0)\n",
    "#qqp = diffQ(ff, [0, 0.5, 0.5], trans_p, alpha)\n",
    "#print qqp\n",
    "beta_Opt = trainModel(Dset, Lset, offset, lam, nnodes, edges, edge_feature, \n",
    "                      source, alpha, beta_init)\n",
    "\n",
    "# train model direclty wtth test set, compare performance with UWRW\n",
    "#beta_Opt = trainModel(Dset_test, Lset_test, offset, lam, nnodes, edges, edge_feature, \n",
    "#                      testSet, alpha, beta_init)\n",
    "\n",
    "print \"Training source set:\\n\", source\n",
    "print \"\\nTrained model parameters:\\n\", beta_Opt\n",
    "\n",
    "\n",
    "#######################################\n",
    "#### Test model performance ###########\n",
    "#######################################\n",
    "\n",
    "print \"Evaluating model performance...\"\n",
    "\n",
    "# link prediction with transition matrices computed with trained parameters\n",
    "ff = genFeatures(nnodes, edges, edge_feature)\n",
    "trans_srw = genTrans(nnodes, edges, ff, testSet, alpha, beta_Opt[0])\n",
    "#trans_srw = genTrans(nnodes, edges, ff, testSet, alpha, [10, 10])\n",
    "\n",
    "# compute personalized PageRank for test nodes to recommend links\n",
    "pgrank_srw = []\n",
    "cand_pairs_srw = []\n",
    "link_hits_srw = []\n",
    "for i in range(len(testSet)):\n",
    "    pp = np.repeat(1.0/nnodes, nnodes)\n",
    "    curpgrank = iterPageRank(pp, trans_srw[i])\n",
    "    # record the pgrank score\n",
    "    pgrank_srw.append(curpgrank)\n",
    "    \n",
    "    # find the top ranking nodes in candidates set\n",
    "    cand_pairs = []\n",
    "    for j in candidates_test[i]:\n",
    "        cand_pairs.append((j, curpgrank[j]))\n",
    "    cand_pairs = sorted(cand_pairs, key = lambda x: x[1], reverse=True)\n",
    "    # record candidate-pagerank pairs\n",
    "    cand_pairs_srw.append(cand_pairs)\n",
    "    \n",
    "    # calculate precision of the top-Dsize_cut predicted links\n",
    "    link_hits = 0    \n",
    "    for j in range(10):\n",
    "        if cand_pairs[j][0] in Dset_test[i]:\n",
    "            link_hits += 1\n",
    "    link_hits_srw.append(link_hits)\n",
    "\n",
    "print \"\\nSRW performance: \", np.mean(link_hits_srw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540669856459\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(len(testSet)):\n",
    "    total += len(Dset_test[i])\n",
    "print sum(link_hits_srw)*1.0/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n",
      "1233\n"
     ]
    }
   ],
   "source": [
    "# Measurement preparation\n",
    "# read in ID_to_Index for conversion\n",
    "import csv\n",
    "import networkx as nx\n",
    "Index_to_ID = {}\n",
    "with open(\"ID_to_Index.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    ID_Index_list = list(reader)\n",
    "f.close()\n",
    "for i in ID_Index_list:\n",
    "    Index_to_ID[i[1]] = i[0]\n",
    "# read in ground truth graph to calculate measurements\n",
    "import csv\n",
    "import networkx as nx\n",
    "with open(\"amazon-meta_filtered_ground_truth.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_list  = list(reader)\n",
    "f.close()\n",
    "    \n",
    "#read in ground truth graph\n",
    "G_ground_truth = nx.Graph()\n",
    "all_nodes_inorder = [];\n",
    "for t in testing_list:\n",
    "    all_nodes_inorder.append(t[0]);\n",
    "\n",
    "    for i in range(len(t)):\n",
    "        source = t[i]\n",
    "        for j in range(i+1,len(t)):\n",
    "            target = t[j]\n",
    "            G_ground_truth.add_node(source)\n",
    "            G_ground_truth.add_node(target)\n",
    "            G_ground_truth.add_edge(source,target)\n",
    "print len(G_ground_truth.nodes())\n",
    "print(len(G_ground_truth.edges()))\n",
    "\n",
    "cand_pairs_srw_all = []\n",
    "for i in range(len(cand_pairs_srw)):\n",
    "    for j in range(len(cand_pairs_srw[i])):\n",
    "        cand_pairs_srw_all.append((testSet[i],cand_pairs_srw[i][j][0],cand_pairs_srw[i][j][1]))\n",
    "\n",
    "cand_pairs_srw_all_sort = sorted(cand_pairs_srw_all,key=lambda x:-x[2])\n",
    "cand_pairs_partial = cand_pairs_srw_all_sort[:len(G_ground_truth.edges())]\n",
    "\n",
    "# generate similarity graph\n",
    "G_sim = nx.Graph()\n",
    "for c in cand_pairs_partial:\n",
    "    G_sim.add_node(Index_to_ID[str(c[0])])\n",
    "    G_sim.add_node(Index_to_ID[str(c[1])])\n",
    "    G_sim.add_edge(Index_to_ID[str(c[0])],Index_to_ID[str(c[1])])\n",
    "    \n",
    "# derive classifications of ground truth and SRW\n",
    "compare = []\n",
    "testing_list = list(set(G_sim.nodes()).intersection(set(G_ground_truth.nodes())))\n",
    "for node1 in testing_list:\n",
    "    for node2 in testing_list:\n",
    "        if node1 != node2:\n",
    "            tmp = [node1,node2]\n",
    "            if not nx.has_path(G_ground_truth,node1,node2):\n",
    "                tmp.append(-1)\n",
    "            elif nx.shortest_path_length(G_ground_truth,node1,node2) == 1:\n",
    "                tmp.append(1)\n",
    "            else:\n",
    "                tmp.append(0)\n",
    "            if not nx.has_path(G_sim,node1,node2):\n",
    "                tmp.append(-1)\n",
    "            elif nx.shortest_path_length(G_sim,node1,node2) == 1:\n",
    "                tmp.append(1)\n",
    "            else:\n",
    "                tmp.append(0)\n",
    "            compare.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Measurement 1', 0.8646528535078153)\n"
     ]
    }
   ],
   "source": [
    "# Measurement 1, classification accuracy\n",
    "accuracy = 0\n",
    "for c in compare:\n",
    "    if c[2] == c[3]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy*1.0 / len(compare)\n",
    "print('Measurement 1',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Measurement 2', 0.2118380062305296)\n"
     ]
    }
   ],
   "source": [
    "# Measurement 2, label 1 classification accuracy\n",
    "accuracy = 0\n",
    "total_1 = 0\n",
    "for c in compare:\n",
    "    if c[2] == c[3] and c[3] == 1:\n",
    "        accuracy += 1\n",
    "    if c[2] == 1:\n",
    "        total_1 += 1\n",
    "print('Measurement 2',accuracy*1.0 / total_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model performance...\n",
      "0.489667374381\n"
     ]
    }
   ],
   "source": [
    "# Measurement #3 and #4 \n",
    "# change value of K to 5 or 10 \n",
    "\n",
    "K = 5\n",
    "accuracy = []\n",
    "\n",
    "print \"Evaluating model performance...\"\n",
    "\n",
    "# link prediction with transition matrices computed with trained parameters\n",
    "ff = genFeatures(nnodes, edges, edge_feature)\n",
    "trans_srw = genTrans(nnodes, edges, ff, testSet, alpha, beta_Opt[0])\n",
    "#trans_srw = genTrans(nnodes, edges, ff, testSet, alpha, [10, 10])\n",
    "\n",
    "# compute personalized PageRank for test nodes to recommend links\n",
    "pgrank_srw = []\n",
    "cand_pairs_srw = []\n",
    "link_hits_srw = []\n",
    "for i in range(len(testSet)):\n",
    "    pp = np.repeat(1.0/nnodes, nnodes)\n",
    "    curpgrank = iterPageRank(pp, trans_srw[i])\n",
    "    # record the pgrank score\n",
    "    pgrank_srw.append(curpgrank)\n",
    "    \n",
    "    # find the top ranking nodes in candidates set\n",
    "    cand_pairs = []\n",
    "    for j in candidates_test[i]:\n",
    "        cand_pairs.append((j, curpgrank[j]))\n",
    "    cand_pairs = sorted(cand_pairs, key = lambda x: x[1], reverse=True)\n",
    "    # record candidate-pagerank pairs\n",
    "    cand_pairs_srw.append(cand_pairs)\n",
    "    \n",
    "    # calculate precision of the top-Dsize_cut predicted links\n",
    "    link_hits = 0    \n",
    "    for j in range(K):\n",
    "        if cand_pairs[j][0] in Dset_test[i]:\n",
    "            link_hits += 1\n",
    "    link_hits_srw.append(link_hits)\n",
    "\n",
    "#print \"\\nSRW performance: \", np.mean(link_hits_srw)\n",
    "import numpy\n",
    "accuracy = []\n",
    "for i in range(len(Dset_test)):\n",
    "    if len(Dset_test[i]) > 0:\n",
    "        accuracy.append(link_hits_srw[i]*1.0 / len(Dset_test[i]))\n",
    "print numpy.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
