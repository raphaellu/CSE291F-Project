{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/collinli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#import igraph\n",
    "import csv\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "#with open(\"testing_set.txt\", \"r\") as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    testing_set  = list(reader)\n",
    "\n",
    "#testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calcualte inner product\n",
    "def inner(X,Y):\n",
    "    s = 0\n",
    "    for i in range(0,len(X)):\n",
    "        s = s + X[i]*Y[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"amazon-meta_item_item_0.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"amazon-meta_item_info.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f,delimiter='^')\n",
    "    node_info  = list(reader)\n",
    "\n",
    "node_info = node_info[1:]\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "title = [element[1] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amazon-meta_filtered_ground_truth.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \")[0] for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list = []\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i]\n",
    "    for j in xrange(len(testing_set)):\n",
    "        target = testing_set[j]\n",
    "        testing_list.append([source,target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set))))\n",
    "training_set_reduced_total = [training_set[i] for i in to_keep]\n",
    "#create a local test set\n",
    "training_set_reduced = training_set_reduced_total[:len(training_set_reduced_total)/2]\n",
    "testing_set = training_set_reduced_total[len(training_set_reduced_total)/2:]\n",
    "#training_set_reduced = training_set_reduced_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# title TFIDF cos similarity\n",
    "title_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# calculate title TFIDF cos similarity\n",
    "    source_title_tfidf = features_TFIDF[index_source].toarray()[0]\n",
    "    target_title_tfidf = features_TFIDF[index_target].toarray()[0]\n",
    "    \n",
    "    cos = np.dot(source_title_tfidf,target_title_tfidf) / (np.linalg.norm(np.array(source_title_tfidf))\\\n",
    "       *np.linalg.norm(np.array(target_title_tfidf)))\n",
    "    title_similarity.append(cos)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# group is the same or not\n",
    "group_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    #group similarity true or false\n",
    "    group_source = source_info[2]\n",
    "    group_target = target_info[2]\n",
    "    \n",
    "    if group_source == group_target:\n",
    "        group_similarity.append(1)\n",
    "    else:\n",
    "        group_similarity.append(0)\n",
    "        \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# number of reviews\n",
    "reviews_num_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # number of reviews similarity\n",
    "    review_source = source_info[4]\n",
    "    review_target = target_info[4]\n",
    "    \n",
    "    reviews_num_similarity.append(np.absolute(int(review_source)-int(review_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# same number of words in detailed category\n",
    "category_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # same number of words in detailed category\n",
    "    category_source = source_info[6+int(review_source)*4:]\n",
    "    category_target = target_info[6+int(review_target)*4:]\n",
    "    \n",
    "    category_similarity.append(len(set(category_source).intersection(set(category_target))))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "10001 training examples processsed\n",
      "20001 training examples processsed\n",
      "30001 training examples processsed\n",
      "40001 training examples processsed\n",
      "50001 training examples processsed\n",
      "60001 training examples processsed\n",
      "70001 training examples processsed\n",
      "80001 training examples processsed\n",
      "90001 training examples processsed\n",
      "100001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# rating \n",
    "rating_similarity = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set_reduced)):\n",
    "#for i in range(1):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # rating similarity\n",
    "    rating_source = source_info[5]\n",
    "    rating_target = target_info[5]\n",
    "    \n",
    "    rating_similarity.append(np.absolute(float(rating_source)-float(rating_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array([title_similarity, group_similarity, category_similarity,reviews_num_similarity,rating_similarity]).T\n",
    "#training_features = np.array([overlap_title_tm, temp_diff_tm, comm_auth_tm, text_mining]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random prediction\n",
    "predictions_random = np.random.choice([0, 1], size=len(tmp_test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.04050700e-01  -1.38777878e-17   8.49054927e-02   1.02180752e-02\n",
      "  -1.85777161e-02]\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "theta = np.linalg.lstsq(training_features, labels_array)[0]\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "40001 testing examples processsed\n",
      "50001 testing examples processsed\n",
      "60001 testing examples processsed\n",
      "70001 testing examples processsed\n",
      "80001 testing examples processsed\n",
      "90001 testing examples processsed\n",
      "100001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "title_similarity_test = []\n",
    "group_similarity_test = []\n",
    "reviews_num_similarity_test = []\n",
    "category_similarity_test = []\n",
    "rating_similarity_test = []\n",
    "   \n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "#for i in range(50):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# calculate title TFIDF cos similarity\n",
    "    source_title_tfidf = features_TFIDF[index_source].toarray()[0]\n",
    "    target_title_tfidf = features_TFIDF[index_target].toarray()[0]\n",
    "    \n",
    "    cos = np.dot(source_title_tfidf,target_title_tfidf) / (np.linalg.norm(np.array(source_title_tfidf))\\\n",
    "       *np.linalg.norm(np.array(target_title_tfidf)))\n",
    "    title_similarity_test.append(cos)\n",
    "    \n",
    "    #group similarity true or false\n",
    "    group_source = source_info[2]\n",
    "    group_target = target_info[2]\n",
    "    \n",
    "    if group_source == group_target:\n",
    "        group_similarity_test.append(1)\n",
    "    else:\n",
    "        group_similarity_test.append(0)\n",
    "        \n",
    "    # number of reviews similarity\n",
    "    review_source = source_info[4]\n",
    "    review_target = target_info[4]\n",
    "    \n",
    "    reviews_num_similarity_test.append(np.absolute(int(review_source)-int(review_target)))\n",
    "    \n",
    "    # same number of words in detailed category\n",
    "    category_source = source_info[6+int(review_source)*4:]\n",
    "    category_target = target_info[6+int(review_target)*4:]\n",
    "    \n",
    "    category_similarity_test.append(len(set(category_source).intersection(set(category_target))))\n",
    "    \n",
    "    # rating similarity\n",
    "    rating_source = source_info[5]\n",
    "    rating_target = target_info[5]\n",
    "    \n",
    "    rating_similarity_test.append(np.absolute(float(rating_source)-float(rating_target)))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"testing examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array([title_similarity_test, group_similarity_test, category_similarity_test,reviews_num_similarity_test,rating_similarity_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction SVM\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "#predictions = zip(range(len(testing_set)), predictions_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction Linear\n",
    "predictions_linear = []\n",
    "for i in xrange(len(testing_features)):\n",
    "#for i in range(50):\n",
    "    r = inner(theta,testing_features[i])\n",
    "    #if r >= 0:\n",
    "    #    predictions_linear.append(1)\n",
    "    #else:\n",
    "    #    predictions_linear.append(0)\n",
    "    predictions_linear.append(r)\n",
    "predictions = zip(range(len(testing_set)), predictions_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726116547339\n"
     ]
    }
   ],
   "source": [
    "result = [int(t[2]) for t in testing_set]\n",
    "compare = zip(result,predictions_SVM)\n",
    "accuracy = 0\n",
    "for c in compare:\n",
    "    if c[0] == c[1]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy*1.0 / len(compare)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "\n",
    "with open(\"improved_predictions.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(['id','category'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
